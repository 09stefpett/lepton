import numpy as np
import os
import sys
import argparse
cpp_header = """
template<typename T> float FUNCTION_NAME(const T & vals_in, int index) {
  static_assert(T::size0 >= INSERT_WEIGHTS1_SIZE,
     "array must be sized to hold at least INSERT_WEIGHTS1_SIZE");
  static const float bias1[INSERT_WEIGHTS2_SIZE] = { INSERT_BIAS1 };
  static const float weights1[][INSERT_WEIGHTS2_SIZE] = { INSERT_WEIGHTS1 };
  static const float bias2[OUTPUT_SIZE] = { INSERT_BIAS2 };
  static const float weights2[][OUTPUT_SIZE] = { INSERT_WEIGHTS2 };
  always_assert(index < OUTPUT_SIZE);
  float tmp[INSERT_WEIGHTS2_SIZE];
  for (unsigned int i = 0; i < INSERT_WEIGHTS2_SIZE; i++) {
    float sum = 0.f;
    for (unsigned int j = 0; j < INSERT_WEIGHTS1_SIZE; j++) {
      sum += weights1[j][i] * vals_in[j];
    }
    sum += bias1[i];
    tmp[i] = sum > 0.f ? sum : 0.f; // RELU
  }
  float sum = 0.f;
  for (unsigned int j = 0; j < INSERT_WEIGHTS2_SIZE; j++) {
    sum += weights2[j][index] * tmp[j];
  }
  sum += bias2[index];
  return sum;
}
float specialized_FUNCTION_NAME(const Sirikata::Array1d<int16_t, INSERT_WEIGHTS1_SIZE> &v,
                                int index) {
  return FUNCTION_NAME(v, index);
}
static int assign_FUNCTION_NAME() {
  global_gen_lepton_prior.at(LEPTON_OFFSET_COORDS) = &specialized_FUNCTION_NAME;
  return 0;
}
static int initialize_FUNCTION_NAME = assign_FUNCTION_NAME();
"""

def array_to_cstring(arry):
    return ','.join(str(x) for x in arry)

def export_into_cpp2(fc1, bias1, fc2, bias2, coords):

    str_coords = [str(coord) for coord in coords]
    fname = 'gen_lepton_prior_' + '_'.join(str_coords)
    template = cpp_header
    template = fname.join(template.split('FUNCTION_NAME'))
    template = template.replace('LEPTON_OFFSET_COORDS',
                                ','.join(str_coords))
    rows1, rows2 = fc1.shape[0:2]#get_shape()[0:2]
    print rows1, rows2
    str_weight1 = ''
    for j in range(rows1):
        str_weight1 += '{' + array_to_cstring(fc1[j]) + '}\n'
        if j != rows1-1:
            str_weight1 += ',\n'
    str_weight2 = ''
    for j in range(rows2):
        str_weight2 += '{' + array_to_cstring(fc2[j]) + '}\n'
        if j != rows2-1:
            str_weight2 += ',\n'

    template = str(len(fc2[0])).join(template.split('OUTPUT_SIZE'))
    template = array_to_cstring(bias1).join(template.split('INSERT_BIAS1'))
    template = array_to_cstring(bias2).join(template.split('INSERT_BIAS2'))
    template = str(rows1).join(template.split('INSERT_WEIGHTS1_SIZE'))
    template = str(rows2).join(template.split('INSERT_WEIGHTS2_SIZE'))
    template = str_weight1.join(template.split('INSERT_WEIGHTS1'))
    template = str_weight2.join(template.split('INSERT_WEIGHTS2'))
    return template

sys.path.append('/usr/local/lib/python2.7/site-packages')

import tensorflow as tf

parser = argparse.ArgumentParser(
    description="Run a 2-layer neural network on priors generated by lepton, "
                "compare with lepton compression as baseline.")
parser.add_argument("input_file", help="Input file to run model on")
parser.add_argument("output_header", help="File to output cpp headers into")
parser.add_argument("--test_file", default="", help="Input file to validate model against")
parser.add_argument("--output_file", default="", help="File to output results into")
parser.add_argument('--k', default=3, type=int, choices=[1, 2, 3, 4, 5, 6, 7, 8],
                    help="How many of the first n values to condition on")
parser.add_argument('--branch_index', default=-1, type=int,
                    help="Which of the branches (in sorted order) to execute the model on.")
parser.add_argument('--rows_to_read', default=400000, type=int,
                    help="How many rows of the input file(s) to read")
parser.add_argument("--c1_size",default=8,type=int, help="Size of the middle layer of the perceptron.")
parser.add_argument("--num_steps",default=2500, type=int, help="Num steps to train model")
parser.add_argument("--min_samples",default=5000,type=int,
                    help="Num samples for a bucket considered enough to train")
args = parser.parse_args()

INPUT_FILE = args.input_file
TEST_FILE = args.test_file
OUTPUT_FILE = args.output_file
OUTPUT_HEADER = args.output_header
K = args.k
BRANCH_INDEX = args.branch_index
ROWS_TO_READ = args.rows_to_read
MIN_SAMPLES = args.min_samples
NUM_STEPS = args.num_steps
C1 = args.c1_size

def to_one_hot(vec, max_enum_plus_one=None):
    if max_enum_plus_one is None:
        max_enum_plus_one = np.max(vec) + 1
    retvals = []
    for i in range(max_enum_plus_one):
        retvals.append(vec == i)

    result = np.concatenate(retvals,1)
    print result.shape
    return result

def read_short(x):
    return ord(x[0]) + ord(x[1]) * 256

def load_file(data_file_name, max_rows=None, shuffle=False):
    with open(data_file_name, 'r') as f:
        output_per_row = read_short(f.read(2))
        f.read(2 * (output_per_row - 1))
        stride_raw = f.read(2)
        stride = read_short(stride_raw)
        assert stride == 86
        print 'Found stride %d' % stride

        stride_extra = f.read(2 * (stride - output_per_row - 1))
        assert stride_extra == stride_raw * (stride - output_per_row - 1)

        raw_data = np.fromfile(f, dtype=np.int16)
        print len(raw_data)
        print len(raw_data)%stride, len(raw_data)/stride
        if max_rows is not None:
            raw_data = raw_data[:stride * max_rows]
        max_rows = len(raw_data)/stride
        input_vbit_number = np.arange(max_rows)
        raw_data = np.concatenate([np.reshape(input_vbit_number,
                                        (max_rows, 1)),
                                   np.reshape(raw_data, (len(raw_data) / stride, stride))],1)

        print raw_data.shape
        if shuffle:
            np.random.seed(0)
            np.random.shuffle(raw_data)

        input_vbit_number, input_bitval, lepton_ones, lepton_zeros, bucket_index, input_prior \
            = np.split(raw_data, [1,2,3,4,5], 1)
        assert 1 == np.amax(np.amax(input_bitval))
        assert 0 == np.amin(np.amin(input_bitval))
        input_lepton_prob = np.add(lepton_ones, 1).astype('float32') / np.add(lepton_zeros, np.add(lepton_ones, 2)).astype('float32')
    return input_vbit_number, input_bitval, input_lepton_prob, input_prior
input_vbit_number, input_bitval, input_lepton_prob, input_prior = load_file(INPUT_FILE, ROWS_TO_READ, True)
if TEST_FILE:
    test_vbit_number,  test_bitval,  test_lepton_prob,  test_prior  = load_file(TEST_FILE,  ROWS_TO_READ, False)


    print 'bitval',input_bitval.shape, test_bitval.shape
    print 'prob',input_lepton_prob.shape, test_lepton_prob.shape
    print 'prior',input_prior.shape, test_prior.shape


stride = input_prior.shape[1]
print 'stride',stride

# We'll condition on the first K values
branches = list(set(tuple([r[i] for i in range(K)]) for r in input_prior[:, :K]))
branches.sort()

print '%d unique branches for the first %d fields' % (len(branches), K)
input = tf.placeholder(tf.float32, shape=[None, stride - K])   # four nearby blocks
output = tf.placeholder(tf.float32, shape=[None, 1])       # 0 or 1? or soft prob?

with tf.name_scope('hellotf'):
    c0 = stride - K
    c1 = C1
    c2 = 1
    n_params = c0 * c1 + c1 + c1 * c2 + c2
    print 'Num params: %d' % n_params
    fc1 = tf.Variable(tf.truncated_normal([c0,c1], stddev=1.0/8.0), name='weights1')
    bias1 = tf.Variable(tf.zeros([c1]), name='bias1')
    layer1 = tf.matmul(input, fc1) + bias1
    fc2 = tf.Variable(tf.truncated_normal([c1,c2], stddev=1.0/8.0), name='weights2')
    bias2 = tf.Variable(tf.zeros([c2]), name='bias2')
    layer2 = tf.matmul(tf.nn.relu(layer1), fc2) + bias2

    # Really the loss is the entropy, e.g. if output=1, -log2(layer2) or -log2(1.0-layer2) otherwise
    layer2_capped = tf.clip_by_value(layer2, 0.01, 0.99)
    entropy_loss = ((output - 1.0) * tf.log(1.0-layer2_capped) - output * tf.log(layer2_capped)) / np.log(2)
    loss = tf.reduce_mean(entropy_loss, name='loss')
    optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)


#actually do the training
running_costs = [0, 0, 0]
test_running_costs = [0, 0, 0]
max_prob = 254./255.;
min_prob = 1./255.;
if TEST_FILE:
    ofp = open(OUTPUT_FILE,'wb')
oheader = open(OUTPUT_HEADER, 'wb')

branch_indices = [BRANCH_INDEX]
if BRANCH_INDEX == -1:
    branch_indices = range(len(branches))

for branch_index in branch_indices:
    branch = branches[branch_index]

    # First filter the input to only select the samples that belong to this branch.
    indices = [i for i in range(input_bitval.size) if (input_prior[i,:K] == branch).all()]

    if len(indices) < MIN_SAMPLES:
        continue

    filtered_bitval = input_bitval[indices, :]
    filtered_lepton_prob = input_lepton_prob[indices, :]
    filtered_prior = input_prior[indices, K:].astype('float32')
    # Clamp prob to avoid entropy going nan
    filtered_lepton_prob = np.minimum(np.maximum(filtered_lepton_prob, min_prob), max_prob)
    if TEST_FILE:
        test_indices = [i for i in range(test_bitval.size) if (test_prior[i,:K] == branch).all()]
        filtered_test_bitval = test_bitval[test_indices,:]
        filtered_test_lepton_prob = test_lepton_prob[test_indices,:]
        filtered_test_prior = test_prior[test_indices, K:].astype('float32')
        filtered_test_vbit_indices = test_vbit_number[test_indices,:].astype('int32')
        filtered_test_lepton_prob = np.minimum(np.maximum(filtered_test_lepton_prob, min_prob), max_prob)
    print 'Training for %r (%d samples) (%d tests)' % (branch, len(indices), len(test_indices) if TEST_FILE else 0)

    session = tf.InteractiveSession()
    tf.initialize_all_variables().run()
    batch_size = 512
    n_training_cutoff = int(len(indices) * 0.8)

    cost_lepton = np.sum(filtered_bitval[n_training_cutoff:] * (-np.log2(filtered_lepton_prob[n_training_cutoff:])) +
        (1 - filtered_bitval[n_training_cutoff:]) * (-np.log2(1 - filtered_lepton_prob[n_training_cutoff:])))
    if TEST_FILE:
        cost_test_lepton = np.sum(filtered_test_bitval * (-np.log2(filtered_test_lepton_prob)) +
            (1 - filtered_test_bitval) * (-np.log2(1 - filtered_test_lepton_prob)))


    for step in range(NUM_STEPS):
        offset = (step * batch_size) % (n_training_cutoff - batch_size)
        batch_input = filtered_prior[offset:(offset + batch_size), :]
        batch_groundtruth = filtered_bitval[offset:(offset + batch_size), :]
        feed_dict = {input : batch_input, output : batch_groundtruth}
        _, l = session.run([optimizer, loss], feed_dict=feed_dict)
        if (step + 1) % 200 == 0:
            print ' Loss at step=%d: %f' % (step, l)
            predicted_prob = (tf.matmul(tf.nn.relu(tf.matmul(filtered_prior[n_training_cutoff:], fc1) + bias1), fc2) + bias2).eval()
            predicted_prob = np.minimum(np.maximum(predicted_prob, min_prob), max_prob)

            cost_tf = np.sum(filtered_bitval[n_training_cutoff:] * (-np.log2(predicted_prob)) +
                (1 - filtered_bitval[n_training_cutoff:]) * (-np.log2(1 - predicted_prob)))


    print 'Bits needed: %f (lepton) vs %f (tf)' % (cost_lepton, cost_tf)
    running_costs[0] += cost_lepton
    running_costs[1] += cost_tf
    running_costs[2] += min(cost_tf, cost_lepton)
    print fc1, fc2
    if oheader.tell() == 0:
        oheader.write('#include "nn_framework.hh"\n')
    oheader.write(export_into_cpp2(fc1.eval(session), bias1.eval(session), fc2.eval(session), bias2.eval(session), branch))
    oheader.flush()
    if TEST_FILE:
        test_predicted_prob = (tf.matmul(tf.nn.relu(tf.matmul(filtered_test_prior, fc1) + bias1), fc2) + bias2).eval()
        #for i in range(len(filtered_test_vbit_indices)):
        #    print [filtered_test_vbit_indices[i], test_predicted_prob[i], filtered_test_prior[i]]
        np.concatenate([filtered_test_vbit_indices, (test_predicted_prob * 255.0).astype('int32')], 1).tofile(ofp)
        ofp.flush()
        test_predicted_prob = np.minimum(np.maximum(test_predicted_prob, min_prob), max_prob)
        cost_tf_diff_file = np.sum(filtered_test_bitval * (-np.log2(test_predicted_prob)) +
                                   (1 - filtered_test_bitval) * (-np.log2(1-test_predicted_prob)))
    else:
        cost_test_lepton = 0
        cost_tf_diff_file = 0
    print ' Training loss: %f (vs %f) or %f vs (%f on separate file)' % (cost_tf, cost_lepton, cost_tf_diff_file if TEST_FILE else 0, cost_test_lepton if TEST_FILE else 0)

    print 'Cumulative: %f vs %f vs %f' % (running_costs[0], running_costs[1], running_costs[2])
    print 'Bits needed: %f (separate_lepton) vs %f (separate tf)' % (cost_test_lepton, cost_tf_diff_file)
    if TEST_FILE:
        test_running_costs[0] += cost_test_lepton
        test_running_costs[1] += cost_tf_diff_file
        test_running_costs[2] += min(cost_tf_diff_file, cost_test_lepton)
        print 'CumualSepa: %f vs %f vs %f' % (test_running_costs[0], test_running_costs[1], test_running_costs[2])
if TEST_FILE:
    ofp.close()
